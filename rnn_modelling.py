# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AQxE2yxBGZRIngwGzMLHwDrpsIKqKyRO
"""

import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt # Visualization
import matplotlib.dates as mdates # Formatting dates
import seaborn as sns # Visualization
from sklearn.preprocessing import MinMaxScaler
import torch # Library for implementing Deep Neural Network
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from graphing import Stocks

import yfinance as yf
from datetime import date, timedelta, datetime

def loadPrices(fn):
    global nt, nInst
    df = pd.read_csv(fn, sep='\s+', header=None, index_col=None)
    (nt, nInst) = df.shape
    return (df.values).T

end_date = date.today().strftime("%Y-%m-%d") #end date for our data retrieval will be current date
start_date = '1990-01-01' # Beginning date for our historical data retrieval

#df = yf.download('AAPL', start=start_date, end=end_date)# Function used to fetch the data

def data_plot(df):
	df_plot = df.copy()

	ncols = 2
	nrows = int(round(df_plot.shape[1] / ncols, 0))

	fig, ax = plt.subplots(nrows=nrows, ncols=ncols,
						sharex=True, figsize=(14, 7))
	for i, ax in enumerate(fig.axes):
		sns.lineplot(data=df_plot.iloc[:, i], ax=ax)
		ax.tick_params(axis="x", rotation=30, labelsize=10, length=0)
		ax.xaxis.set_major_locator(mdates.AutoDateLocator())
	fig.tight_layout()
	plt.show()

nt = 0
nInst = 50
file_path = './prices.txt'
prcAll = loadPrices(file_path)

#data_plot(df)
df = Stocks(prcAll)
df = df.data[df.data['Stock'] == 0]
# Set 'Day' as the index
df.set_index('Day', inplace=True)

# Extract only the 'Price' column and create a new DataFrame
df = df[['Price']].iloc[0:401]

print(df)
"""

train_end = 400
test_end = train_end + 100

# Extract training and testing data
train_data = df.iloc[:train_end]
test_data = df.iloc[train_end:test_end]
"""
# Define the split points
training_data_len = math.ceil(len(df) * .8)

train_data = df[:training_data_len].iloc[:,:1]
test_data = df[training_data_len:].iloc[:,:1]

train_data.shape, test_data.shape

dataset_train = train_data.Price.values
dataset_test = test_data.Price.values
# reshaping 1d to 2d array
dataset_train = np.reshape(dataset_train, (-1,1))
dataset_test = np.reshape(dataset_test, (-1,1))
dataset_train.shape, dataset_test.shape

# normalisation
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
# scaling dataset
scaled_train = scaler.fit_transform(dataset_train)
print(*scaled_train[:5])
scaled_test = scaler.fit_transform(dataset_test)
print(*scaled_test[:5])

# transforming data into sequence
# training data
sequence_length = 50
x_train, y_train = [], []
for i in range(len(scaled_train) - sequence_length):
    x_train.append(scaled_train[i:i+sequence_length])
    y_train.append(scaled_train[i+1:i+sequence_length+1])
x_train, y_train = np.array(x_train), np.array(y_train)

# convert to pytorch tensors
x_train = torch.tensor(x_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
x_train.shape, y_train.shape

# testing data
sequence_length = 30
x_test, y_test = [], []
for i in range(len(scaled_test) - sequence_length):
    x_test.append(scaled_test[i:i+sequence_length])
    y_test.append(scaled_test[i+1:i+sequence_length+1])
x_test, y_test = np.array(x_test), np.array(y_test)

x_test = torch.tensor(x_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)
x_test.shape, y_test.shape

class LSTMModel(nn.Module):
    # input_size: number of features in input at each time step
    # hidden_size: number of LSTM units
    # num_layers: number of LSTM layers
    def __init__(self, input_size, hidden_size, num_layers):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.linear(out)
        return out

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
device

input_size = 1
num_layers = 2
hidden_size = 64
output_size = 1

#define the model, loss function and optimiser
model = LSTMModel(input_size, hidden_size, num_layers).to(device)

loss_fn = torch.nn.MSELoss(reduction='mean')
optimiser = torch.optim.Adam(model.parameters(), lr=1e-2)
model

batch_size = 16
# create datalaoder for batch training
train_dataset = torch.utils.data.TensorDataset(x_train, y_train)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# creating dataloader for batch training
test_dataset = torch.utils.data.TensorDataset(x_test, y_test)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

num_epochs = 50
train_hist = []
test_hist = []

for epoch in range(num_epochs):
    total_loss = float(0.0)

    # training
    model.train()
    for batch_x, batch_y in train_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        predictions = model(batch_x)
        loss = loss_fn(predictions, batch_y)

        optimiser.zero_grad()
        loss.backward()
        optimiser.step()

        total_loss += loss.item()

    # calculate average training loss and accuracy
    average_loss = total_loss / len(train_loader)
    train_hist.append(average_loss)

    # validation on test data
    model.eval()
    with torch.no_grad():
        total_test_loss = float(0.0)

        for batch_x_test, batch_y_test in test_loader:
            batch_x_test, batch_y_test = batch_x_test.to(device), batch_y_test.to(device)
            predictions_test = model(batch_x_test)
            test_loss = loss_fn(predictions_test, batch_y_test)

            total_test_loss += test_loss.item()

        average_test_loss = total_test_loss / len(test_loader)
        test_hist.append(average_test_loss)

    if (epoch+1) % 10 == 0:
        print('Epoch [{}/{}] - Training Loss: {:.4f}, Test Loss: {:.4f}'.format(epoch+1, num_epochs, average_loss, average_test_loss))

"""
x = np.linspace(1,num_epochs, num_epochs)
plt.plot(x, train_hist, label="Training loss")
plt.plot(x, test_hist, label="Test Loss")
plt.legend()
plt.show()
"""


# define the number of future time steps to forcast
num_forecast_steps = 100

# convert to numpy and remove singleton dimensions
sequence_to_plot = x_test.squeeze().cpu().numpy()

# use the last 30 data points as the starting point
historical_data = sequence_to_plot[-1]
# print(historical_data.shape)

# initialise a list to store the forecasted values
forecasted_values = []

# use the trained model to forecast future values
with torch.no_grad():
    for _ in range(num_forecast_steps):
        # prepare historical_data tensor
        historical_data_tensor = torch.as_tensor(historical_data).view(1, -1, 1).float().to(device)
        # use the model to predict the next value
        predicted_value = model(historical_data_tensor).cpu().numpy()[0, 0]

        # append the predicted value to the forecasted_value list
        forecasted_values.append(predicted_value[0])

        # update the historical_data sequence by removing the oldest value and adding the predicted value
        historical_data = np.roll(historical_data, shift=-1)
        historical_data[-1] = predicted_value

"""
# generate future dates
last_date = test_data.index[-1]

# Generate a range of days from last_date + 1 to last_date + num_forecast_steps
future_days = range(last_date + 1, last_date + num_forecast_steps + 1)

# Convert future_days to a pandas Index
future_index = pd.Index(future_days)

# Combine the existing index with the new future index
combined_index = test_data.index.append(future_index)

plt.figure(figsize=(14,4))
print(test_data.shape)
print(combined_index.shape)
# test data
plt.plot(test_data.index, test_data.Price, label="test_data", color="b")
# reverse the scaling transformation
original_cases = scaler.inverse_transform(np.expand_dims(sequence_to_plot[-1], axis=0)).flatten()

# the historical data used as input for forecasting
plt.plot(test_data.index, original_cases[:100], label = "actual values", color="green")

# forecasted values
# reverse the scaling transformation
forecasted_cases = scaler.inverse_transform(np.expand_dims(forecasted_values, axis=0)).flatten()
# plotting the forecasted values
plt.plot(combined_index[-100:], forecasted_cases, label="forecasted values", color="red")

plt.xlabel('Time Step')
plt.ylabel('Value')
plt.legend()
plt.title('Time Series Forecasting')
plt.grid(True)
plt.show()
"""
# Generate future dates
# Generate future dates
last_date = test_data.index[-1]
future_days = range(last_date + 1, last_date + num_forecast_steps + 1)
future_index = pd.Index(future_days)
combined_index = test_data.index.append(future_index)

# Plot the test data and forecasted values
plt.figure(figsize=(14, 4))
plt.subplot(2,1,1)
plt.plot(test_data.index, test_data.Price, label="Test Data", color="b")
original_cases = scaler.inverse_transform(np.expand_dims(sequence_to_plot[-1], axis=0)).flatten()
plt.plot(test_data.index[-30:], original_cases, label="Actual Values", color="green")
forecasted_cases = scaler.inverse_transform(np.expand_dims(forecasted_values, axis=0)).flatten()
plt.plot(combined_index[-num_forecast_steps:], forecasted_cases, label="Forecasted Values", color="red")
plt.xlabel('Day')
plt.ylabel('Price')
plt.legend()
plt.title('Time Series Forecasting')
plt.grid(True)

plt.subplot(2,1,2)
plt.plot(df.iloc[300:501], label="Actual Data")
plt.grid(True)
plt.show()