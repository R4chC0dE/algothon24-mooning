# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AQxE2yxBGZRIngwGzMLHwDrpsIKqKyRO
"""

import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt # Visualization
import matplotlib.dates as mdates # Formatting dates
import seaborn as sns # Visualization
from sklearn.preprocessing import MinMaxScaler
import torch # Library for implementing Deep Neural Network
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
from graphing import Stocks

import yfinance as yf
from datetime import date, timedelta, datetime

def loadPrices(fn):
    global nt, nInst
    df = pd.read_csv(fn, sep='\s+', header=None, index_col=None)
    (nt, nInst) = df.shape
    return (df.values).T

end_date = date.today().strftime("%Y-%m-%d") #end date for our data retrieval will be current date
start_date = '1990-01-01' # Beginning date for our historical data retrieval

df = yf.download('AAPL', start=start_date, end=end_date)# Function used to fetch the data

def data_plot(df):
	df_plot = df.copy()

	ncols = 2
	nrows = int(round(df_plot.shape[1] / ncols, 0))

	fig, ax = plt.subplots(nrows=nrows, ncols=ncols,
						sharex=True, figsize=(14, 7))
	for i, ax in enumerate(fig.axes):
		sns.lineplot(data=df_plot.iloc[:, i], ax=ax)
		ax.tick_params(axis="x", rotation=30, labelsize=10, length=0)
		ax.xaxis.set_major_locator(mdates.AutoDateLocator())
	fig.tight_layout()
	plt.show()

nt = 0
nInst = 50
file_path = './prices.txt'
prcAll = loadPrices(file_path)

#data_plot(df)
df = Stocks(prcAll)
df = df.data[df.data['Stock'] == 1]
# Set 'Day' as the index
df.set_index('Day', inplace=True)

# Extract only the 'Price' column and create a new DataFrame
df = df[['Price']]

training_data_len = math.ceil(len(df) * 400/500)
#print(len(df))

train_data = df[:training_data_len].iloc[:,:1]
test_data = df[training_data_len:].iloc[:,:1]
train_data.shape, test_data.shape
dataset_train = train_data.Price.values
dataset_test = test_data.Price.values
# reshaping 1d to 2d array
dataset_train = np.reshape(dataset_train, (-1,1))
dataset_test = np.reshape(dataset_test, (-1,1))
#dataset_train.shape, dataset_test.shape

# normalisation
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
# scaling dataset
scaled_train = scaler.fit_transform(dataset_train)
print(*scaled_train[:5])
scaled_test = scaler.fit_transform(dataset_test)
print(*scaled_test[:5])

# transforming data into sequence
# training data
sequence_length = 50
x_train, y_train = [], []
for i in range(len(scaled_train) - sequence_length):
    x_train.append(scaled_train[i:i+sequence_length])
    y_train.append(scaled_train[i+1:i+sequence_length+1])
x_train, y_train = np.array(x_train), np.array(y_train)

# convert to pytorch tensors
x_train = torch.tensor(x_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
print(x_train.shape)
print(y_train.shape)

# testing data
sequence_length = 30
x_test, y_test = [], []
for i in range(len(scaled_test) - sequence_length):
    x_test.append(scaled_test[i:i+sequence_length])
    y_test.append(scaled_test[i+1:i+sequence_length+1])
x_test, y_test = np.array(x_test), np.array(y_test)

x_test = torch.tensor(x_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)
x_test.shape, y_test.shape

# Create TensorDataset
train_dataset = TensorDataset(x_train, y_train)

# Create DataLoader
batch_size = 5
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

class LSTMModel(nn.Module):
    # input_size: number of features in input at each time step
    # hidden_size: number of LSTM units
    # num_layers: number of LSTM layers
    def __init__(self, input_size, hidden_size, num_layers):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.linear(out)
        return out

input_size = 1
num_layers = 2
hidden_size = 64
output_size = 1
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

#define the model, loss function and optimiser
model = LSTMModel(input_size, hidden_size, num_layers).to(device)

loss_fn = torch.nn.MSELoss(reduction='mean')
optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)

class LRFinder:
    def __init__(self, model, optimizer, criterion, device):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        self.history = {'lr': [], 'loss': []}

    def range_test(self, train_loader, end_lr=10, num_iter=100, smooth_f=0.05):
        lrs = []
        losses = []

        # Reset the model and optimizer
        self.model.train()
        self.model.to(self.device)
        self.optimizer.param_groups[0]['lr'] = 1e-7
        best_loss = float('inf')

        # Calculate the step size
        step_size = (end_lr / 1e-7) ** (1 / num_iter)

        for i, (inputs, targets) in enumerate(train_loader):
            if i >= num_iter:
                break

            # Move inputs and targets to the device
            inputs, targets = inputs.to(self.device), targets.to(self.device)

            # Zero the gradients
            self.optimizer.zero_grad()

            # Forward pass
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)

            # Backward pass
            loss.backward()
            self.optimizer.step()

            # Track the learning rate and loss
            lrs.append(self.optimizer.param_groups[0]['lr'])
            losses.append(loss.item())

            # Update the learning rate
            self.optimizer.param_groups[0]['lr'] *= step_size

            # Print progress
            print(f'Iteration {i+1}/{num_iter}, Loss: {loss.item()}, LR: {self.optimizer.param_groups[0]["lr"]}')

            # Smooth the loss to reduce noise
            if i == 0:
                smoothed_loss = loss.item()
            else:
                smoothed_loss = smooth_f * loss.item() + (1 - smooth_f) * smoothed_loss

            # Update best loss
            if smoothed_loss < best_loss:
                best_loss = smoothed_loss

            # Stop if the loss explodes
            if smoothed_loss > 4 * best_loss:
                break

        # Record the learning rate and loss history
        self.history['lr'] = lrs
        self.history['loss'] = losses

    def plot(self):
        plt.plot(self.history['lr'], self.history['loss'])
        plt.xscale('log')
        plt.xlabel('Learning Rate')
        plt.ylabel('Loss')
        plt.title('Learning Rate Finder')
        plt.show()

print(f'Number of batches in train_loader: {len(train_loader)}')
# Define the model, loss function, and optimizer
model = LSTMModel(input_size, hidden_size, num_layers).to(device)
criterion = nn.MSELoss(reduction='mean')
optimizer = optim.Adam(model.parameters(), lr=1e-7)  # Starting with a very small learning rate

# Create the learning rate finder
lr_finder = LRFinder(model, optimizer, criterion, device)

# Run the learning rate range test
lr_finder.range_test(train_loader, end_lr=10, num_iter=100)

# Plot the learning rate vs. loss
lr_finder.plot()


